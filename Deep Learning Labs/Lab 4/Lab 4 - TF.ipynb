{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2F1A3tZNFr2"
   },
   "source": [
    "# Text Generation using LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "colab_type": "code",
    "id": "LYDeCjh8M8ac",
    "outputId": "b2d937e7-4e90-4089-bc9c-31c7c4a4b2b4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xxasVSynOJ3n"
   },
   "source": [
    "## Download the data\n",
    "\n",
    "The best place to access books that are no longer under Copyright is [Project Gutenberg](https://www.gutenberg.org/). Today we recommend using [Aliceâ€™s Adventures in Wonderland by Lewis Carroll](https://www.gutenberg.org/files/11/11-0.txt) for consistency. Of course you can experiment with other books as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eqEtoK_RTlbJ"
   },
   "outputs": [],
   "source": [
    "data_url = 'https://www.gutenberg.org/files/219/219-0.txt'\n",
    "fname = 'heart_of_darkness.txt'\n",
    "\n",
    "if fname not in os.listdir():\n",
    "    urllib.request.urlretrieve(data_url, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NvBomIM2VpQX"
   },
   "source": [
    "## Load data and create character to integer mappings\n",
    "\n",
    "- Open the text file, read the data then convert it to lowercase letters.\n",
    "- Map each character to a respective number. Keep 2 dictionaries in order to have more easily access to the mappings both ways around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTrVy-7kWc8V"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "def read_doc(document):\n",
    "    with open (document, encoding='UTF-8') as f:\n",
    "        return f.read().lower()\n",
    "\n",
    "# Characters to integers\n",
    "def chars(text):\n",
    "\n",
    "    txt = ''\n",
    "    \n",
    "    for word in text:\n",
    "        for char in word:\n",
    "            txt += char\n",
    "    \n",
    "    char_to_idx = {}\n",
    "    idx_to_char = {}\n",
    "    for i, char in enumerate(set(txt)):\n",
    "        char_to_idx[char] = i + 1\n",
    "        idx_to_char[i+1] = char\n",
    "    \n",
    "    return char_to_idx, idx_to_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = os.path.join(os.getcwd(), fname)\n",
    "text = read_doc(doc_path)\n",
    "\n",
    "char_to_idx, idx_to_char = chars(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1ZroU78YtLk"
   },
   "source": [
    "## Prepare the data\n",
    "- We are \"thinking\" in sequences of 100 characters: 99 characters in the input and 1 in the output.  \n",
    "E.g. for the sequence *\\['h', 'e', 'l', 'l'\\]* as input, we will have *\\['o'\\]* as the expected output.\n",
    "- Reshape X such that it has the shape expected by a LSTM: \\[samples, time steps, features\\].\n",
    "  - samples: number of data points (len(X));\n",
    "  - time steps: number of time-dependent steps that are in a single data point (100);\n",
    "  - features: number of variables for the true value in Y (1).\n",
    "- Scale the values in X to be in \\[0, 1\\].\n",
    "- One-hot encode the true values in Y_modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzMC31hFaGuo"
   },
   "outputs": [],
   "source": [
    "# Initialize the input and output with empty lists\n",
    "seq_x = []\n",
    "seq_y = []\n",
    "n_chars = len(text)\n",
    "for i in range(0, n_chars - 100, 1):\n",
    "    # Consider sequences of 99 characters starting from i\n",
    "    input = text[i:i+100]\n",
    "    # The 100th character is the label\n",
    "    output = text[i+100]\n",
    "\n",
    "    # Append to the input the list of ints corresponding to the characters in the current sequence\n",
    "    seq_x.append([char_to_idx[char] for char in input])\n",
    "    # Append to the output the int corresponding to the label (as list)\n",
    "    seq_y.append(char_to_idx[output])\n",
    "# Re-shape the inputs\n",
    "X = np.array(seq_x).reshape(len(seq_x), 100, 1)\n",
    "# Scale the inputs\n",
    "X_norm = (X.astype('float32') / 49.)\n",
    "# One-hot encode labels\n",
    "y = np_utils.to_categorical(seq_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_U8znKQesIV"
   },
   "source": [
    "## Define the LSTM model\n",
    "\n",
    "- Instantiate the model: a linear stack of layers.\n",
    "- First layer: LSTM with 256 memory units, input shape from X_new (1st and 2nd). Make sure that this layer returns sequences, such that the next LSTM layer receives sequences and not just random data.\n",
    "- Second layer: dropout 20% of the neurons of the previous layer in order to avoid overfitting.\n",
    "\n",
    "****** \n",
    "Optional:\n",
    "- Third layer: LSTM(256).\n",
    "- Fourth layer: dropout 20% of the neurons.\n",
    "******\n",
    "- Last layer: fully connected with a 'softmax' activation function, and as many neurons as the number of unique characters (the output is one-hot encoded).\n",
    "\n",
    "\n",
    "Compile the model: categorical_crossentropy, adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "pprKER_wev5k",
    "outputId": "8be3699d-15c1-47b7-d217-d04ae080edf8"
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = Sequential()\n",
    "# Add LSTM layer\n",
    "model.add(LSTM(256, return_sequences=True, input_shape=(X_norm.shape[1], X_norm.shape[2])))\n",
    "# Add dropout\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dropout(0.2))\n",
    "# Add another LSTM layer\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "# Add dropout\n",
    "model.add(Dropout(0.2))\n",
    "# Add a Dense layer\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cMAFfAGchLCd"
   },
   "source": [
    "## Train the model and generate characters\n",
    "\n",
    "Fit the model for over 100 epochs as the batch size is 30 (ideally). In this case, given the time constraints, we are going to use 5 epochs and a batch size of 128. \n",
    "\n",
    "Fix a random seed and start generating characters.  The prediction from the model gives out the character encoding of the predicted character, it is then decoded back to the character value and appended to the pattern.  \n",
    "\n",
    "After enough training time it is going to look like something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "YCqV31YahVVA",
    "outputId": "bf4c7b1c-7206-4e97-9e5f-29519f5bf979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6991/6991 [==============================] - 222s 31ms/step - loss: 2.8483 - accuracy: 0.2058\n",
      "Epoch 2/50\n",
      "6991/6991 [==============================] - 217s 31ms/step - loss: 2.4101 - accuracy: 0.3128\n",
      "Epoch 3/50\n",
      "6991/6991 [==============================] - 219s 31ms/step - loss: 2.2201 - accuracy: 0.3632\n",
      "Epoch 4/50\n",
      "6991/6991 [==============================] - 219s 31ms/step - loss: 2.0936 - accuracy: 0.3943\n",
      "Epoch 5/50\n",
      "6991/6991 [==============================] - 220s 31ms/step - loss: 2.0011 - accuracy: 0.4179\n",
      "Epoch 6/50\n",
      "6991/6991 [==============================] - 220s 31ms/step - loss: 1.9285 - accuracy: 0.4354\n",
      "Epoch 7/50\n",
      "6991/6991 [==============================] - 217s 31ms/step - loss: 1.8700 - accuracy: 0.4508\n",
      "Epoch 8/50\n",
      "6991/6991 [==============================] - 220s 31ms/step - loss: 1.8193 - accuracy: 0.4628\n",
      "Epoch 9/50\n",
      "6991/6991 [==============================] - 218s 31ms/step - loss: 1.7765 - accuracy: 0.4736\n",
      "Epoch 10/50\n",
      "6991/6991 [==============================] - 218s 31ms/step - loss: 1.7394 - accuracy: 0.4827\n",
      "Epoch 11/50\n",
      "6991/6991 [==============================] - 217s 31ms/step - loss: 1.7105 - accuracy: 0.4903\n",
      "Epoch 12/50\n",
      "6991/6991 [==============================] - 216s 31ms/step - loss: 1.6792 - accuracy: 0.4986\n",
      "Epoch 13/50\n",
      "6991/6991 [==============================] - 216s 31ms/step - loss: 1.6541 - accuracy: 0.5033\n",
      "Epoch 14/50\n",
      "6991/6991 [==============================] - 219s 31ms/step - loss: 1.6320 - accuracy: 0.5096\n",
      "Epoch 15/50\n",
      "6991/6991 [==============================] - 218s 31ms/step - loss: 1.6130 - accuracy: 0.5128\n",
      "Epoch 16/50\n",
      "6991/6991 [==============================] - 219s 31ms/step - loss: 1.5932 - accuracy: 0.5188\n",
      "Epoch 17/50\n",
      "6991/6991 [==============================] - 217s 31ms/step - loss: 1.5772 - accuracy: 0.5208\n",
      "Epoch 18/50\n",
      "6991/6991 [==============================] - 220s 31ms/step - loss: 1.5615 - accuracy: 0.5267\n",
      "Epoch 19/50\n",
      "6991/6991 [==============================] - 217s 31ms/step - loss: 1.5452 - accuracy: 0.5314\n",
      "Epoch 20/50\n",
      "6991/6991 [==============================] - 216s 31ms/step - loss: 1.5895 - accuracy: 0.5210\n",
      "Epoch 21/50\n",
      "6991/6991 [==============================] - 220s 31ms/step - loss: 1.5118 - accuracy: 0.5396\n",
      "Epoch 22/50\n",
      "6991/6991 [==============================] - 220s 31ms/step - loss: 1.5109 - accuracy: 0.5385\n",
      "Epoch 23/50\n",
      "6991/6991 [==============================] - 218s 31ms/step - loss: 1.5032 - accuracy: 0.5414\n",
      "Epoch 24/50\n",
      "6991/6991 [==============================] - 216s 31ms/step - loss: 1.4928 - accuracy: 0.5437\n",
      "Epoch 25/50\n",
      "6991/6991 [==============================] - 215s 31ms/step - loss: 1.4836 - accuracy: 0.5458\n",
      "Epoch 26/50\n",
      "6991/6991 [==============================] - 217s 31ms/step - loss: 1.4759 - accuracy: 0.5469\n",
      "Epoch 27/50\n",
      "6991/6991 [==============================] - 219s 31ms/step - loss: 1.4653 - accuracy: 0.5508\n",
      "Epoch 28/50\n",
      "6991/6991 [==============================] - 216s 31ms/step - loss: 1.4558 - accuracy: 0.5523\n",
      "Epoch 29/50\n",
      "6991/6991 [==============================] - 218s 31ms/step - loss: 1.4463 - accuracy: 0.5551\n",
      "Epoch 30/50\n",
      "6991/6991 [==============================] - 220s 31ms/step - loss: 1.4407 - accuracy: 0.5561\n",
      "Epoch 31/50\n",
      "6991/6991 [==============================] - 214s 31ms/step - loss: 1.4342 - accuracy: 0.5567\n",
      "Epoch 32/50\n",
      "6991/6991 [==============================] - 208s 30ms/step - loss: 1.4237 - accuracy: 0.5603\n",
      "Epoch 33/50\n",
      "6991/6991 [==============================] - 208s 30ms/step - loss: 1.4181 - accuracy: 0.5618\n",
      "Epoch 34/50\n",
      "6991/6991 [==============================] - 208s 30ms/step - loss: 1.4127 - accuracy: 0.5632\n",
      "Epoch 35/50\n",
      "6991/6991 [==============================] - 208s 30ms/step - loss: 1.4062 - accuracy: 0.5647\n",
      "Epoch 36/50\n",
      "6991/6991 [==============================] - 207s 30ms/step - loss: 1.4014 - accuracy: 0.5668\n",
      "Epoch 37/50\n",
      "6991/6991 [==============================] - 205s 29ms/step - loss: 1.3982 - accuracy: 0.5670\n",
      "Epoch 38/50\n",
      "6991/6991 [==============================] - 206s 29ms/step - loss: 1.3875 - accuracy: 0.5712\n",
      "Epoch 39/50\n",
      "6991/6991 [==============================] - 205s 29ms/step - loss: 1.3823 - accuracy: 0.5716\n",
      "Epoch 40/50\n",
      "6991/6991 [==============================] - 205s 29ms/step - loss: 1.3768 - accuracy: 0.5733\n",
      "Epoch 41/50\n",
      "6991/6991 [==============================] - 206s 29ms/step - loss: 1.3749 - accuracy: 0.5728\n",
      "Epoch 42/50\n",
      "6991/6991 [==============================] - 205s 29ms/step - loss: 1.3688 - accuracy: 0.5754\n",
      "Epoch 43/50\n",
      "6991/6991 [==============================] - 205s 29ms/step - loss: 1.3637 - accuracy: 0.5753\n",
      "Epoch 44/50\n",
      "6991/6991 [==============================] - 205s 29ms/step - loss: 1.3612 - accuracy: 0.5754\n",
      "Epoch 45/50\n",
      "6991/6991 [==============================] - 207s 30ms/step - loss: 1.3542 - accuracy: 0.5789\n",
      "Epoch 46/50\n",
      "6991/6991 [==============================] - 207s 30ms/step - loss: 1.3511 - accuracy: 0.5801\n",
      "Epoch 47/50\n",
      "6991/6991 [==============================] - 206s 29ms/step - loss: 1.3469 - accuracy: 0.5796\n",
      "Epoch 48/50\n",
      "6991/6991 [==============================] - 206s 29ms/step - loss: 1.3431 - accuracy: 0.5807\n",
      "Epoch 49/50\n",
      "6991/6991 [==============================] - 207s 30ms/step - loss: 1.3392 - accuracy: 0.5827\n",
      "Epoch 50/50\n",
      "6991/6991 [==============================] - 209s 30ms/step - loss: 1.3376 - accuracy: 0.5815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f64dc830d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=2, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_norm, y, epochs=50, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "n-5fYO89h8SC",
    "outputId": "edae4c70-bd32-42bd-9aa6-916eb71c5cf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" lies within the shell of a cracked nut. but marlow was not typical (if\n",
      "his propensity to spin yarns  \"\n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "tt\n",
      "ii\n",
      "oo\n",
      "nn\n",
      "  \n",
      "ww\n",
      "aa\n",
      "ss\n",
      "  \n",
      "aa\n",
      "  \n",
      "ll\n",
      "ii\n",
      "tt\n",
      "tt\n",
      "ll\n",
      "ee\n",
      "  \n",
      "tt\n",
      "aa\n",
      "cc\n",
      "kk\n",
      "  \n",
      "oo\n",
      "ff\n",
      "  \n",
      "tt\n",
      "hh\n",
      "ee\n",
      "  \n",
      "ss\n",
      "tt\n",
      "aa\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "begin = np.random.randint(0, len(seq_x)-1)\n",
    "pattern = seq_x[begin]\n",
    "print(\"\\\"\", ''.join([idx_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "for i in range(1000):\n",
    "\t\n",
    "\t#predict\n",
    "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(len(char_to_idx))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = idx_to_char[index]\n",
    "\tseq_in = [idx_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "\tprint(result)\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXBJeBijfiH0"
   },
   "source": [
    "# Bonus: Words as features\n",
    "\n",
    "Code here:\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/ "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab4_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Deep_Learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "4be34ee29682d8153bb33aa0ed218e251bc723c8d0fb2453deb0a4cf51f72620"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
